# Query Plan Analysis: DuckDB vs Spark 4.0

**Date:** 2025-12-29
**Data:** TPC-H SF1.0 (~1GB, 1.5M orders, 6M line items)

---

## Executive Summary

This document compares the physical query plans generated by **DuckDB** (via Thunderduck) and **Apache Spark 4.0** for identical DataFrame operations. Both engines apply sophisticated optimizations, but use fundamentally different execution strategies.

### Key Differences

| Aspect | DuckDB | Spark 4.0 |
|--------|--------|-----------|
| **Execution Model** | Single-node, vectorized | Distributed (local[*] mode) |
| **Join Strategy** | Hash joins exclusively | Sort-merge joins (default) |
| **Shuffle** | None (single-node) | Hash partitioning exchanges |
| **Aggregation** | Single-pass hash | Two-phase (partial + final) |
| **TOP-N** | Heap-based, late materialization | TakeOrderedAndProject |

---

## Example 1: Basic Aggregation

### PySpark
```python
spark.table("orders")
    .groupBy("o_orderstatus")
    .agg(F.count("*").alias("order_count"))
    .orderBy("o_orderstatus")
```

### DuckDB Physical Plan
```
Total Time: 0.014s

*(3) Sort [o_orderstatus ASC]
    └── HASH_GROUP_BY (3 rows, 0.01s)
            └── PROJECTION [o_orderstatus]
                    └── TABLE_SCAN [READ_PARQUET] (1.5M rows, 0.01s)
                          Projections: o_orderstatus
```

### Spark 4.0 Physical Plan
```
*(3) Sort [o_orderstatus ASC NULLS FIRST], true, 0
+- Exchange rangepartitioning(o_orderstatus ASC, 4), ENSURE_REQUIREMENTS
   +- *(2) HashAggregate(keys=[o_orderstatus], functions=[count(1)])
      +- Exchange hashpartitioning(o_orderstatus, 4), ENSURE_REQUIREMENTS
         +- *(1) HashAggregate(keys=[o_orderstatus], functions=[partial_count(1)])
            +- *(1) ColumnarToRow
               +- FileScan parquet [o_orderstatus]
                    PushedFilters: [], ReadSchema: struct<o_orderstatus:string>
```

### Comparison

| Aspect | DuckDB | Spark |
|--------|--------|-------|
| Aggregation | Single-pass hash | Two-phase (partial → exchange → final) |
| Shuffle | None | 2 exchanges (hash + range partitioning) |
| Stages | 1 | 3 |
| Column Pruning | ✅ Only `o_orderstatus` | ✅ Only `o_orderstatus` |

**Analysis:** Spark's two-phase aggregation with shuffle is designed for distributed execution. On single-node, DuckDB's single-pass approach has less overhead.

---

## Example 2: Filter and Select with LIMIT

### PySpark
```python
lineitem
    .select(l_orderkey, l_partkey, l_quantity, l_extendedprice, l_discount,
            (l_extendedprice * (1 - l_discount)).alias("net_price"))
    .filter(F.col("l_extendedprice") > 50000)
    .orderBy(F.desc("net_price"))
    .limit(20)
```

### DuckDB Physical Plan
```
Total Time: 0.113s

ORDER_BY (20 rows)
    └── PROJECTION [6 columns] (20 rows)
            └── HASH_JOIN [SEMI] (20 rows)
                    ├── TABLE_SCAN [full columns] (39K rows, 0.13s)
                    │     Late materialization: only for TOP-20 rows
                    └── TOP_N [Top 20] (20 rows)
                            └── PROJECTION [net_price, file_index, row_number]
                                    └── TABLE_SCAN [READ_PARQUET] (1.9M rows, 0.27s)
                                          Projections: l_extendedprice, l_discount
                                          Filters: l_extendedprice>50000.00
```

### Spark 4.0 Physical Plan
```
TakeOrderedAndProject(limit=20, orderBy=[net_price DESC NULLS LAST],
                      output=[l_orderkey, l_partkey, l_quantity, l_extendedprice, l_discount, net_price])
+- *(1) Project [l_orderkey, l_partkey, l_quantity, l_extendedprice, l_discount,
                 (l_extendedprice * (1 - l_discount)) AS net_price]
   +- *(1) Filter (isnotnull(l_extendedprice) AND (l_extendedprice > 50000.00))
      +- *(1) ColumnarToRow
         +- FileScan parquet [l_orderkey, l_partkey, l_quantity, l_extendedprice, l_discount]
              DataFilters: [isnotnull(l_extendedprice), (l_extendedprice > 50000.00)]
              PushedFilters: [IsNotNull(l_extendedprice), GreaterThan(l_extendedprice,50000.00)]
              ReadSchema: struct<l_orderkey:bigint,l_partkey:bigint,...>
```

### Comparison

| Aspect | DuckDB | Spark |
|--------|--------|-------|
| Filter Pushdown | ✅ `l_extendedprice>50000` | ✅ `GreaterThan(l_extendedprice,50000.00)` |
| Column Pruning | ✅ 5 columns | ✅ 5 columns |
| TOP-N Strategy | Late materialization (2 scans) | TakeOrderedAndProject (1 scan) |
| Execution | Semi-join for late fetch | Single pass with heap |

**Analysis:** DuckDB uses an interesting late materialization strategy - first scan finds TOP-20 row positions using only sort columns, second scan fetches full rows. Spark uses TakeOrderedAndProject which maintains a heap during single scan.

---

## Example 3: Multi-table Join with Aggregation

### PySpark
```python
customer
    .join(orders, customer.c_custkey == orders.o_custkey)
    .join(lineitem, orders.o_orderkey == lineitem.l_orderkey)
    .join(nation, customer.c_nationkey == nation.n_nationkey)
    .groupBy(nation.n_name.alias("nation"))
    .agg(F.sum(lineitem.l_extendedprice * (1 - lineitem.l_discount)).alias("revenue"))
    .orderBy(F.desc("revenue"))
    .limit(10)
```

### DuckDB Physical Plan
```
Total Time: 0.146s

TOP_N [Top 10] (10 rows)
    └── HASH_GROUP_BY [sum] (25 rows, 0.06s)
            └── PROJECTION [n_name, revenue_expr] (6M rows)
                    └── HASH_JOIN [l_orderkey = o_orderkey] (6M rows, 0.20s)
                            ├── TABLE_SCAN [lineitem] (6M rows, 0.17s)
                            │     Projections: l_orderkey, l_extendedprice, l_discount
                            └── HASH_JOIN [o_custkey = c_custkey] (1.5M rows)
                                    ├── TABLE_SCAN [orders] (1.5M rows, 0.07s)
                                    │     Projections: o_orderkey, o_custkey
                                    └── HASH_JOIN [c_nationkey = n_nationkey] (150K rows)
                                            ├── TABLE_SCAN [customer] (150K rows)
                                            │     Filters: c_custkey<=149999
                                            └── TABLE_SCAN [nation] (25 rows)
```

### Spark 4.0 Physical Plan
```
TakeOrderedAndProject(limit=10, orderBy=[revenue DESC NULLS LAST])
+- *(14) HashAggregate(keys=[n_name], functions=[sum(revenue_expr)])
   +- Exchange hashpartitioning(n_name, 4), ENSURE_REQUIREMENTS
      +- *(13) HashAggregate(keys=[n_name], functions=[partial_sum(revenue_expr)])
         +- *(13) Project [l_extendedprice, l_discount, n_name]
            +- *(13) SortMergeJoin [c_nationkey], [n_nationkey], Inner
               :- *(10) Sort [c_nationkey ASC], false, 0
               :  +- Exchange hashpartitioning(c_nationkey, 4)
               :     +- *(9) Project [c_nationkey, l_extendedprice, l_discount]
               :        +- *(9) SortMergeJoin [o_orderkey], [l_orderkey], Inner
               :           :- *(6) Sort [o_orderkey ASC], false, 0
               :           :  +- Exchange hashpartitioning(o_orderkey, 4)
               :           :     +- *(5) Project [c_nationkey, o_orderkey]
               :           :        +- *(5) SortMergeJoin [c_custkey], [o_custkey], Inner
               :           :           :- *(2) Sort [c_custkey ASC], false, 0
               :           :           :  +- Exchange hashpartitioning(c_custkey, 4)
               :           :           :     +- *(1) FileScan parquet [c_custkey, c_nationkey]
               :           :           +- *(4) Sort [o_custkey ASC], false, 0
               :           :              +- Exchange hashpartitioning(o_custkey, 4)
               :           :                 +- *(3) FileScan parquet [o_orderkey, o_custkey]
               :           +- *(8) Sort [l_orderkey ASC], false, 0
               :              +- Exchange hashpartitioning(l_orderkey, 4)
               :                 +- *(7) FileScan parquet [l_orderkey, l_extendedprice, l_discount]
               +- *(12) Sort [n_nationkey ASC], false, 0
                  +- Exchange hashpartitioning(n_nationkey, 4)
                     +- *(11) FileScan parquet [n_nationkey, n_name]
```

### Comparison

| Aspect | DuckDB | Spark |
|--------|--------|-------|
| Join Type | Hash joins (3x) | Sort-merge joins (3x) |
| Shuffles | 0 | 8 exchanges |
| Stages | 1 | 14 |
| Sorts | 0 | 4 (for sort-merge) |
| Aggregation | Single-pass | Two-phase |
| Join Order | nation → customer → orders → lineitem | customer → orders → lineitem → nation |

**Analysis:** Spark uses sort-merge joins (better for distributed execution, more memory-efficient for large tables) while DuckDB uses hash joins (faster on single-node). Spark's 8 shuffle exchanges add overhead in local mode.

---

## Example 4: Window Functions

### PySpark
```python
customer_spending = (
    customer.join(orders, ...).join(nation, ...)
    .groupBy("n_name", "c_custkey", "c_name")
    .agg(F.sum("o_totalprice").alias("total_spending"))
)

window = Window.partitionBy("n_name").orderBy(F.desc("total_spending"))
customer_spending
    .withColumn("rank", F.rank().over(window))
    .filter(F.col("rank") <= 3)
    .select("n_name", "c_name", "total_spending", "rank")
    .orderBy("n_name", "rank")
```

### DuckDB Physical Plan
```
Total Time: 0.087s

ORDER_BY [n_name ASC, rank ASC] (75 rows)
    └── FILTER [rank <= 3] (75 rows)
            └── WINDOW [RANK() OVER (PARTITION BY n_name ORDER BY total_spending DESC)]
                (100K rows, 0.03s)
                    └── HASH_GROUP_BY [n_name, c_custkey, c_name] (100K rows, 0.30s)
                            └── HASH_JOIN [o_custkey = c_custkey] (1.5M rows, 0.01s)
                                    ├── TABLE_SCAN [orders] (1.5M rows, 0.07s)
                                    └── HASH_JOIN [c_nationkey = n_nationkey] (150K rows)
                                            ├── TABLE_SCAN [customer] (150K rows)
                                            └── TABLE_SCAN [nation] (25 rows)
```

### Spark 4.0 Physical Plan
```
*(13) Sort [n_name ASC, rank ASC], true, 0
+- Exchange rangepartitioning(n_name ASC, rank ASC, 4)
   +- *(12) Filter (rank <= 3)
      +- Window [rank(total_spending) windowspecdefinition(n_name, total_spending DESC,
                 specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank]
         +- WindowGroupLimit [n_name], [total_spending DESC], rank(total_spending), 3, Final
            +- *(11) Sort [n_name ASC, total_spending DESC], false, 0
               +- Exchange hashpartitioning(n_name, 4)
                  +- WindowGroupLimit [n_name], [total_spending DESC], rank(total_spending), 3, Partial
                     +- *(10) Sort [n_name ASC, total_spending DESC], false, 0
                        +- *(10) HashAggregate(keys=[n_name, c_custkey, c_name], functions=[sum(o_totalprice)])
                           +- Exchange hashpartitioning(n_name, c_custkey, c_name, 4)
                              +- *(9) HashAggregate(keys=[...], functions=[partial_sum(o_totalprice)])
                                 +- *(9) SortMergeJoin [c_nationkey], [n_nationkey], Inner
                                    :- *(6) Sort [c_nationkey ASC], false, 0
                                    :  +- Exchange hashpartitioning(c_nationkey, 4)
                                    :     +- *(5) SortMergeJoin [c_custkey], [o_custkey], Inner
                                    :        :- [customer scan + exchange + sort]
                                    :        +- [orders scan + exchange + sort]
                                    +- [nation scan + exchange + sort]
```

### Comparison

| Aspect | DuckDB | Spark |
|--------|--------|-------|
| Join Type | Hash joins (2x) | Sort-merge joins (2x) |
| Shuffles | 0 | 6 exchanges |
| Stages | 1 | 13 |
| Window | Single-pass | Two-phase with WindowGroupLimit |
| Optimization | - | WindowGroupLimit pushdown (rank<=3) |

**Analysis:** Spark applies a clever optimization - `WindowGroupLimit` pushes the `rank <= 3` filter into the window computation, pruning rows early. DuckDB computes full window then filters.

---

## Summary: Optimizer Comparison

### Optimizations Applied

| Optimization | DuckDB | Spark 4.0 |
|--------------|--------|-----------|
| Subquery Flattening | ✅ | ✅ |
| Column Pruning | ✅ | ✅ |
| Predicate Pushdown | ✅ | ✅ |
| Join Reordering | ✅ | ✅ (different order) |
| Late Materialization | ✅ (TOP-N) | ❌ |
| Two-Phase Aggregation | ❌ (single-node) | ✅ |
| Sort-Merge Joins | ❌ | ✅ (default) |
| WindowGroupLimit | ❌ | ✅ |
| Hash Partitioning | ❌ (single-node) | ✅ |

### Execution Strategy Summary

**DuckDB (Single-Node Optimized):**
- Hash joins throughout (fast, memory-intensive)
- Single-pass aggregation
- No shuffles (all in memory)
- Vectorized execution
- Late materialization for TOP-N

**Spark (Distributed-Ready):**
- Sort-merge joins (scalable, spill-friendly)
- Two-phase aggregation (partial → shuffle → final)
- Hash partitioning for data distribution
- Stage boundaries at shuffles
- WindowGroupLimit optimization

### Why Spark Uses More Stages

Spark's query planning assumes distributed execution:
1. Each `Exchange` (shuffle) creates a stage boundary
2. Sort-merge joins require sorted inputs (extra sort operators)
3. Two-phase aggregation requires shuffle between phases

In `local[*]` mode, these shuffles are in-memory but still add coordination overhead.

---

## Performance Implications

### For Small-Medium Data (fits in memory)
- DuckDB's single-pass approach wins
- No shuffle overhead
- Hash joins are faster than sort-merge

### For Large Data (distributed/spilling)
- Spark's architecture handles spilling gracefully
- Sort-merge joins use less memory
- Two-phase aggregation enables parallel reduction

### Thunderduck's Sweet Spot
Thunderduck targets workloads where:
1. Data fits on a single large node (up to ~1TB)
2. Queries benefit from DuckDB's vectorized execution
3. No shuffle overhead is needed

---

## Appendix: Generated SQL (Thunderduck)

### Example 1
```sql
SELECT * FROM (
    SELECT "o_orderstatus", COUNT(*) AS "order_count"
    FROM "orders" GROUP BY "o_orderstatus"
) AS subquery_1
ORDER BY "o_orderstatus" ASC NULLS LAST
```

### Example 2
```sql
SELECT * FROM (SELECT * FROM (SELECT * FROM (
    SELECT "l_orderkey", "l_partkey", "l_quantity",
           "l_extendedprice", "l_discount",
           ("l_extendedprice" * (1 - "l_discount")) AS "net_price"
    FROM "lineitem"
) AS subquery_1
WHERE "l_extendedprice" > 50000
) AS subquery_2
ORDER BY "net_price" DESC NULLS LAST
) AS subquery_3 LIMIT 20
```

### Example 3
```sql
SELECT * FROM (SELECT * FROM (
    SELECT "n_name" AS "nation",
           SUM("l_extendedprice" * (1 - "l_discount")) AS "revenue"
    FROM "customer" AS customer
    INNER JOIN "orders" AS orders ON customer."c_custkey" = orders."o_custkey"
    INNER JOIN "lineitem" AS lineitem ON orders."o_orderkey" = lineitem."l_orderkey"
    INNER JOIN "nation" AS nation ON customer."c_nationkey" = nation."n_nationkey"
    GROUP BY "n_name"
) AS subquery_1
ORDER BY "revenue" DESC NULLS LAST
) AS subquery_2 LIMIT 10
```

### Example 4
```sql
SELECT * FROM (SELECT * FROM (
    SELECT "n_name", "c_name", "total_spending", "rank"
    FROM (
        SELECT "n_name", "c_custkey", "c_name", "total_spending",
               RANK() OVER (PARTITION BY "n_name" ORDER BY "total_spending" DESC) AS "rank"
        FROM (
            SELECT "n_name", "c_custkey", "c_name", SUM("o_totalprice") AS "total_spending"
            FROM "customer" AS customer
            INNER JOIN "orders" AS orders ON customer."c_custkey" = orders."o_custkey"
            INNER JOIN "nation" AS nation ON customer."c_nationkey" = nation."n_nationkey"
            GROUP BY "n_name", "c_custkey", "c_name"
        ) AS subquery_1
    ) AS subquery_2
    WHERE "rank" <= 3
) AS subquery_3
ORDER BY "n_name" ASC NULLS LAST, "rank" ASC NULLS LAST
) AS subquery_4
```

**Note:** DuckDB's optimizer completely flattens these nested subqueries - the verbose SQL has no performance impact.
