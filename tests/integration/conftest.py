"""
Pytest configuration for Thunderduck Spark Connect integration tests
"""

import pytest
from pathlib import Path
import os
import socket
import sys
import atexit
import signal
import subprocess
import time

# Add utils to path
sys.path.insert(0, str(Path(__file__).parent / "utils"))


# ------------------------------------------------------------------------------
# Load .env file (generated by setup-differential-testing.sh)
# ------------------------------------------------------------------------------

def _load_env_file():
    """Load environment variables from .env file if it exists.

    Only sets variables that are not already in the environment,
    so explicit env vars always take precedence.
    """
    env_file = Path(__file__).parent / ".env"
    if not env_file.exists():
        return
    for line in env_file.read_text().splitlines():
        line = line.strip()
        if not line or line.startswith("#"):
            continue
        # Strip optional 'export ' prefix
        if line.startswith("export "):
            line = line[len("export "):]
        if "=" not in line:
            continue
        key, _, value = line.partition("=")
        key = key.strip()
        value = value.strip().strip('"').strip("'")
        if key and key not in os.environ:
            os.environ[key] = value

_load_env_file()


# ------------------------------------------------------------------------------
# Port allocation and port-scoped cleanup
# ------------------------------------------------------------------------------

# Track allocated ports for cleanup on exit/signal (set by dual_server_manager fixture)
_allocated_ports = set()

# PID file for external cleanup (run script, next-run stale process detection)
_PID_FILE = Path(__file__).parent / "logs" / ".server-pids"


def _write_pid_file(entries: list[tuple[str, int, int]]):
    """Write server PIDs to file for external cleanup. entries: [(name, port, pid), ...]"""
    _PID_FILE.parent.mkdir(parents=True, exist_ok=True)
    with open(_PID_FILE, 'w') as f:
        for name, port, pid in entries:
            f.write(f"{name}:{port}:{pid}\n")


def _read_pid_file() -> list[tuple[str, int, int]]:
    """Read server PIDs from file. Returns [(name, port, pid), ...]"""
    if not _PID_FILE.exists():
        return []
    entries = []
    for line in _PID_FILE.read_text().strip().split('\n'):
        if ':' in line:
            parts = line.split(':')
            if len(parts) == 3:
                entries.append((parts[0], int(parts[1]), int(parts[2])))
    return entries


def _cleanup_pid_file():
    """Kill processes from PID file and delete it."""
    for name, port, pid in _read_pid_file():
        try:
            os.killpg(os.getpgid(pid), signal.SIGKILL)
        except (ProcessLookupError, PermissionError, OSError):
            pass
    _PID_FILE.unlink(missing_ok=True)


def _get_pid_on_port(port: int) -> int | None:
    """Get the PID of the process listening on a specific port."""
    try:
        result = subprocess.run(
            ["lsof", "-ti", f":{port}"],
            capture_output=True, text=True
        )
        for pid in result.stdout.strip().split('\n'):
            if pid:
                return int(pid)
    except Exception:
        pass
    return None


def _allocate_free_port():
    """Get a free port from the OS."""
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        s.bind(('localhost', 0))
        s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        return s.getsockname()[1]


def _kill_process_on_port(port):
    """Kill the process listening on a specific port (our process only)."""
    try:
        result = subprocess.run(
            ["lsof", "-ti", f":{port}"],
            capture_output=True, text=True
        )
        for pid in result.stdout.strip().split('\n'):
            if pid:
                try:
                    os.kill(int(pid), signal.SIGKILL)
                except (ProcessLookupError, ValueError):
                    pass
    except Exception:
        pass


def _cleanup_allocated_ports():
    """Kill processes on all ports allocated by this test session."""
    for port in _allocated_ports:
        _kill_process_on_port(port)
    _PID_FILE.unlink(missing_ok=True)


def signal_handler(signum, frame):
    """Handle interrupt signals"""
    print(f"\n\nReceived signal {signum}, cleaning up servers on ports {_allocated_ports}...")
    _cleanup_allocated_ports()
    sys.exit(1)


# Register signal handlers
signal.signal(signal.SIGINT, signal_handler)
signal.signal(signal.SIGTERM, signal_handler)

# Register atexit handler as fallback
atexit.register(_cleanup_allocated_ports)


def _release_session_without_shutdown(session):
    """
    Release a SparkSession without shutting down the global thread pool.

    PySpark's session.stop() calls ExecutePlanResponseReattachableIterator.shutdown()
    which destroys the process-global ThreadPoolExecutor shared by ALL sessions.
    shutdown(wait=True) blocks until all pending ReleaseExecute RPCs complete —
    if any target an unresponsive server, it blocks forever while holding a
    class-level RLock. This deadlocks all subsequent sessions in the process.

    This function does the essential cleanup (release server session + close
    gRPC channel) WITHOUT touching the shared thread pool. It also neutralizes
    the session's __del__ so the GC can't trigger shutdown() later.
    """
    try:
        if hasattr(session, 'client') and not session.client.is_closed:
            # Tell the server to release this session's resources
            try:
                session.client.release_session()
            except Exception:
                pass
            # Close the gRPC channel directly, bypassing shutdown()
            try:
                session.client._channel.close()
                session.client._closed = True
            except Exception:
                pass
    except Exception:
        pass

    # Neutralize __del__ so GC can't trigger the shutdown() deadlock.
    # Replace the close() method on this specific client instance with a no-op.
    try:
        session.client.close = lambda: None
    except Exception:
        pass


from dual_server_manager import DualServerManager
from port_utils import is_port_listening as _is_port_listening


@pytest.fixture(scope="module")
def spark_session(spark_thunderduck):
    """
    Module-scoped alias for spark_thunderduck.

    For backward compatibility with tests expecting spark_session.
    """
    return spark_thunderduck


@pytest.fixture(scope="module")
def spark(spark_thunderduck):
    """
    Module-scoped alias for spark_thunderduck.

    This allows tests written for single-server mode to work
    in the dual-server testing environment.
    """
    return spark_thunderduck


# Data path fixtures

@pytest.fixture(scope="session")
def workspace_dir():
    """Path to workspace root directory"""
    return Path(__file__).parent.parent.parent


@pytest.fixture(scope="session")
def tpch_data_dir():
    """Path to TPC-H data directory (co-located with integration tests)"""
    data_dir = Path(__file__).parent / "tpch_sf001"
    if not data_dir.exists():
        pytest.skip(f"TPC-H data not found at {data_dir}. Please ensure data files exist.")
    return data_dir


@pytest.fixture(scope="session")
def tpch_queries_dir():
    """Path to TPC-H queries directory"""
    queries_dir = Path(__file__).parent / "sql" / "tpch_queries"
    if not queries_dir.exists():
        pytest.skip(f"TPC-H queries not found at {queries_dir}")
    return queries_dir


# Utility functions

def load_query(query_num: int, queries_dir: Path) -> str:
    """
    Load a TPC-H query from file

    Args:
        query_num: Query number (1-22)
        queries_dir: Path to queries directory

    Returns:
        Query SQL string
    """
    query_file = queries_dir / f"q{query_num}.sql"
    if not query_file.exists():
        pytest.skip(f"Query file not found: {query_file}")

    with open(query_file, 'r') as f:
        return f.read()


@pytest.fixture
def load_tpch_query(tpch_queries_dir):
    """
    Fixture that returns a function to load TPC-H queries
    """
    def _load(query_num: int) -> str:
        return load_query(query_num, tpch_queries_dir)
    return _load


# Pytest configuration

def pytest_configure(config):
    """Register custom markers"""
    config.addinivalue_line(
        "markers", "tpch: mark test as TPC-H benchmark test"
    )
    config.addinivalue_line(
        "markers", "tpcds: mark test as TPC-DS benchmark test"
    )
    config.addinivalue_line(
        "markers", "slow: mark test as slow running"
    )
    config.addinivalue_line(
        "markers", "dataframe: mark test as using DataFrame API"
    )
    config.addinivalue_line(
        "markers", "sql: mark test as using SQL"
    )
    config.addinivalue_line(
        "markers", "differential: mark test as differential test (Spark vs Thunderduck)"
    )
    config.addinivalue_line(
        "markers", "quick: mark test as quick sanity test"
    )
    config.addinivalue_line(
        "markers", "functions: mark test as DataFrame function parity test"
    )
    config.addinivalue_line(
        "markers", "aggregations: mark test as multi-dimensional aggregation test"
    )
    config.addinivalue_line(
        "markers", "window: mark test as window function test"
    )
    config.addinivalue_line(
        "markers", "skip_relaxed: skip test in relaxed compatibility mode"
    )
    config.addinivalue_line(
        "markers", "skip_strict: skip test in strict compatibility mode"
    )
    config.addinivalue_line(
        "markers", "conditional: mark test as conditional expression test"
    )
    config.addinivalue_line(
        "markers", "datetime: mark test as datetime function test"
    )
    config.addinivalue_line(
        "markers", "joins: mark test as join test"
    )


def pytest_collection_modifyitems(config, items):
    """Add markers automatically based on test names and handle mode-specific skipping"""
    compat_mode = os.environ.get('THUNDERDUCK_COMPAT_MODE', 'auto').lower()

    for item in items:
        # Add tpch marker to tests with 'tpch' in name
        if 'tpch' in item.nodeid.lower():
            item.add_marker(pytest.mark.tpch)

        # Add dataframe marker to tests with 'dataframe' in name
        if 'dataframe' in item.nodeid.lower():
            item.add_marker(pytest.mark.dataframe)

        # Add sql marker to tests with 'sql' in name
        if '_sql' in item.nodeid.lower():
            item.add_marker(pytest.mark.sql)

        # Mode-specific skipping
        if compat_mode in ('relaxed', 'auto'):
            marker = item.get_closest_marker('skip_relaxed')
            if marker:
                reason = marker.kwargs.get('reason', 'Skipped in relaxed mode')
                item.add_marker(pytest.mark.skip(reason=reason))

        if compat_mode == 'strict':
            marker = item.get_closest_marker('skip_strict')
            if marker:
                reason = marker.kwargs.get('reason', 'Skipped in strict mode')
                item.add_marker(pytest.mark.skip(reason=reason))


# TPC-DS Fixtures

@pytest.fixture(scope="session")
def tpcds_data_dir():
    """
    Path to TPC-DS test data (co-located with integration tests).

    Auto-generates TPC-DS data at scale factor 0.01 if the directory doesn't exist.
    This ensures tests can run without manual data setup.
    """
    data_dir = Path(__file__).parent / "tpcds_sf001"
    if not data_dir.exists():
        print(f"\nTPC-DS data not found at {data_dir}")
        print("Auto-generating TPC-DS data (SF=0.01)...")

        # Run the generation script
        workspace_root = Path(__file__).parent.parent.parent
        gen_script = workspace_root / "scripts" / "generate_tpcds_via_duckdb.py"

        if not gen_script.exists():
            pytest.skip(f"TPC-DS generation script not found at {gen_script}")

        try:
            result = subprocess.run(
                ["python3", str(gen_script), "--sf", "0.01", "--output", str(data_dir)],
                check=True,
                capture_output=True,
                text=True,
                timeout=300  # 5 minute timeout for data generation
            )
            print(result.stdout)
            print("TPC-DS data generation complete!")
        except subprocess.CalledProcessError as e:
            print(f"TPC-DS generation failed: {e.stderr}")
            pytest.skip(f"TPC-DS data generation failed: {e.stderr}")
        except subprocess.TimeoutExpired:
            pytest.skip("TPC-DS data generation timed out after 5 minutes")

    return data_dir


@pytest.fixture(scope="session")
def tpcds_queries_dir():
    """Path to TPC-DS queries directory"""
    queries_dir = Path(__file__).parent / "sql" / "tpcds_queries"
    if not queries_dir.exists():
        pytest.skip(f"TPC-DS queries not found at {queries_dir}")
    return queries_dir


@pytest.fixture
def load_tpcds_query(tpcds_queries_dir):
    """Load TPC-DS query by number or variant name"""
    def _load_query(qnum):
        """
        Load a TPC-DS query.

        Args:
            qnum: Query number (1-99) or variant string ('14a', '14b', '23a', etc.)
        """
        query_file = tpcds_queries_dir / f"q{qnum}.sql"
        if not query_file.exists():
            pytest.skip(f"Query file not found: {query_file}")
        return query_file.read_text()
    return _load_query


# ============================================================================
# Differential Testing Fixtures (Spark Reference vs Thunderduck)
# ============================================================================
# Consolidated infrastructure with:
# - Low timeouts to detect deadlocks/blocking/failures fast
# - Timing measurements for connect/query plan/collect phases
# - Error classification (hard errors vs soft errors)
# - Diagnostic collection on failures
#
# Configuration via environment variables:
#   SPARK_PORT=15003              - Spark Reference server port
#   THUNDERDUCK_PORT=15002        - Thunderduck server port
#   CONNECT_TIMEOUT=10            - Session creation timeout (seconds)
#   QUERY_PLAN_TIMEOUT=5          - Query plan building timeout
#   COLLECT_TIMEOUT=10            - Result collection timeout
#   HEALTH_CHECK_TIMEOUT=2        - Server health check timeout
#   SERVER_STARTUP_TIMEOUT=60     - Server startup timeout
#   SPARK_MEMORY=4g               - Spark driver memory
#   THUNDERDUCK_MEMORY=2g         - Thunderduck JVM heap
#   THUNDERDUCK_TEST_SUITE_CONTINUE_ON_ERROR=true  - Continue on hard errors
# ============================================================================

from utils.test_orchestrator import TestOrchestrator
from utils.exceptions import HardError


def _get_orchestrator_config():
    """Build orchestrator config from environment variables."""
    return {
        # Ports
        'spark_port': int(os.environ.get('SPARK_PORT', 15003)),
        'thunderduck_port': int(os.environ.get('THUNDERDUCK_PORT', 15002)),
        # Timeouts (low by default to detect issues fast)
        'connect_timeout': int(os.environ.get('CONNECT_TIMEOUT', 10)),
        'query_plan_timeout': int(os.environ.get('QUERY_PLAN_TIMEOUT', 5)),
        'collect_timeout': int(os.environ.get('COLLECT_TIMEOUT', 10)),
        'health_check_timeout': int(os.environ.get('HEALTH_CHECK_TIMEOUT', 2)),
        'server_startup_timeout': int(os.environ.get('SERVER_STARTUP_TIMEOUT', 60)),
        # Memory
        'spark_memory': os.environ.get('SPARK_MEMORY', '4g'),
        'thunderduck_memory': os.environ.get('THUNDERDUCK_MEMORY', '2g'),
        # Behavior
        'continue_on_error': os.environ.get('THUNDERDUCK_TEST_SUITE_CONTINUE_ON_ERROR', '').lower() == 'true',
        # Workspace
        'workspace_dir': str(Path(__file__).parent.parent.parent),
    }


@pytest.fixture(scope="session")
def dual_server_manager():
    """
    Session-scoped fixture that starts both servers for differential testing.

    Starts:
    - Apache Spark Connect 4.1.1 (reference) on SPARK_PORT or auto-allocated port
    - Thunderduck Connect (test) on THUNDERDUCK_PORT or auto-allocated port

    When env vars are not set, ports are auto-allocated from the OS, enabling
    parallel test runs without manual port configuration.

    Both servers are started fresh and killed on teardown (even on interrupt).
    """
    global _allocated_ports

    # Kill orphan servers from crashed previous runs (reads stale PID file)
    _cleanup_pid_file()

    compat_mode = os.environ.get('THUNDERDUCK_COMPAT_MODE', None)
    td_port = int(os.environ.get('THUNDERDUCK_PORT', 0)) or _allocate_free_port()
    spark_port = int(os.environ.get('SPARK_PORT', 0)) or _allocate_free_port()

    # Register ports for signal/atexit cleanup
    _allocated_ports.add(td_port)
    _allocated_ports.add(spark_port)

    # Kill only processes on our specific ports (not all Java processes)
    _kill_process_on_port(td_port)
    _kill_process_on_port(spark_port)

    manager = DualServerManager(
        thunderduck_port=td_port,
        spark_reference_port=spark_port,
        compat_mode=compat_mode
    )

    print("\n" + "="*80)
    print("Starting DUAL servers for differential testing...")
    print(f"  Thunderduck port: {td_port}")
    print(f"  Spark Reference port: {spark_port}")
    print("="*80)

    spark_ok, thunderduck_ok = manager.start_both(timeout=120)

    if not (spark_ok and thunderduck_ok):
        manager.stop_both()
        pytest.exit("Failed to start both servers for differential testing", returncode=1)

    # Write PID file for external cleanup (run script, next-run orphan detection)
    pid_entries = []
    td_pid = (manager.thunderduck_manager.process.pid
              if manager.thunderduck_manager.process else None)
    spark_pid = _get_pid_on_port(spark_port)
    if td_pid:
        pid_entries.append(("thunderduck", td_port, td_pid))
    if spark_pid:
        pid_entries.append(("spark", spark_port, spark_pid))
    if pid_entries:
        _write_pid_file(pid_entries)

    # Report compatibility mode
    requested_mode = compat_mode or 'auto'
    log_file = Path(__file__).parent / "logs" / "server_stderr.log"
    extension_loaded = False
    if log_file.exists():
        log_content = log_file.read_text()
        extension_loaded = "extension loaded" in log_content.lower()
    actual_mode = "strict (extension loaded)" if extension_loaded else "relaxed (no extension)"
    print(f"  Compat mode: requested={requested_mode}, actual={actual_mode}")

    yield manager

    print("\n" + "="*80)
    print("Stopping both servers...")
    print("="*80)
    manager.stop_both()
    _PID_FILE.unlink(missing_ok=True)


@pytest.fixture(scope="session")
def orchestrator(dual_server_manager):
    """
    Session-scoped orchestrator that manages sessions with monitoring.

    Provides:
    - Low timeouts to detect deadlocks/blocking/failures fast
    - Timing measurements for performance analysis
    - Diagnostic collection on hard errors

    Depends on dual_server_manager to ensure servers are running.
    Uses actual allocated ports from the server manager (which may differ
    from env vars when auto-allocation is used).
    """
    config = _get_orchestrator_config()
    # Override with actual allocated ports from the server manager
    config['thunderduck_port'] = dual_server_manager.thunderduck_port
    config['spark_port'] = dual_server_manager.spark_reference_port
    orch = TestOrchestrator(config)

    yield orch

    # Print timing summary at end of test session
    print(orch.timings.get_summary())


# Primary session fixtures (module-scoped to reduce session churn)
@pytest.fixture(scope="module")
def spark_reference(orchestrator, dual_server_manager):
    """
    Module-scoped Spark session connected to Apache Spark Connect (reference).

    This is the reference implementation (official Apache Spark 4.1.1).
    One session per test module reduces session creation overhead (143 -> ~36 pairs).
    Includes health check with auto-restart before session creation.
    """
    # Health check: verify Spark Reference server is responsive
    port = orchestrator.spark_port
    if not _is_port_listening(port):
        print(f"\n  [spark_reference] Server unhealthy (port {port} not listening), restarting...")
        dual_server_manager.stop_spark_reference()
        time.sleep(1)
        if not dual_server_manager.start_spark_reference(timeout=60):
            pytest.fail("Failed to restart Spark Reference server")

    session = orchestrator.create_spark_session()
    yield session
    _release_session_without_shutdown(session)
    orchestrator._active_sessions.discard(session)


@pytest.fixture(scope="module")
def spark_thunderduck(orchestrator, dual_server_manager):
    """
    Module-scoped Spark session connected to Thunderduck Connect (test).

    This is the system under test (Thunderduck implementation).
    One session per test module reduces session creation overhead.
    Includes health check with auto-restart before session creation.
    """
    # Health check: verify Thunderduck server is responsive
    port = orchestrator.thunderduck_port
    if not _is_port_listening(port):
        print(f"\n  [spark_thunderduck] Server unhealthy (port {port} not listening), restarting...")
        dual_server_manager.stop_thunderduck()
        time.sleep(1)
        if not dual_server_manager.start_thunderduck(timeout=60):
            pytest.fail("Failed to restart Thunderduck server")

    session = orchestrator.create_thunderduck_session()
    yield session
    _release_session_without_shutdown(session)
    orchestrator._active_sessions.discard(session)


# Function-scoped sessions (for tests that need per-test isolation)
@pytest.fixture
def spark_reference_isolated(orchestrator):
    """
    Function-scoped: fresh PySpark session per test.

    Use for tests that modify state (temp views, settings) and need
    a clean session for each test.
    """
    session = orchestrator.create_spark_session()
    yield session
    _release_session_without_shutdown(session)
    orchestrator._active_sessions.discard(session)


@pytest.fixture
def thunderduck_isolated(orchestrator):
    """
    Function-scoped: fresh PySpark session per test.

    Use for tests that modify state (temp views, settings) and need
    a clean session for each test.
    """
    session = orchestrator.create_thunderduck_session()
    yield session
    _release_session_without_shutdown(session)
    orchestrator._active_sessions.discard(session)


# Fresh server fixtures (kills and restarts server)
@pytest.fixture
def fresh_spark_server(orchestrator):
    """
    Restarts Spark Reference server, returns new session.

    Use when you need a completely fresh server state.
    WARNING: This is slow (~60s). Only use when absolutely necessary.
    """
    orchestrator.restart_spark_server()
    session = orchestrator.create_spark_session()
    yield session
    _release_session_without_shutdown(session)


@pytest.fixture
def fresh_thunderduck_server(orchestrator):
    """
    Restarts Thunderduck server, returns new session.

    Use when you need a completely fresh server state.
    WARNING: This is slow (~30s). Only use when absolutely necessary.
    """
    orchestrator.restart_thunderduck_server()
    session = orchestrator.create_thunderduck_session()
    yield session
    _release_session_without_shutdown(session)


# ============================================================================
# TPC-H Differential Testing Fixtures
# ============================================================================

@pytest.fixture(scope="module")
def tpch_tables_reference(spark_reference, tpch_data_dir):
    """
    Load TPC-H tables into Spark Reference session (module-scoped).
    """
    tables = [
        'lineitem', 'orders', 'customer', 'part',
        'supplier', 'partsupp', 'nation', 'region'
    ]

    print("\nLoading TPC-H tables into Spark Reference...")
    for table in tables:
        parquet_path = tpch_data_dir / f"{table}.parquet"
        if not parquet_path.exists():
            pytest.skip(f"TPC-H table not found: {parquet_path}")

        df = spark_reference.read.parquet(str(parquet_path))
        df.createOrReplaceTempView(table)

    print(f"✓ All {len(tables)} TPC-H tables loaded into Spark Reference")
    return tables


@pytest.fixture(scope="module")
def tpch_tables_thunderduck(spark_thunderduck, tpch_data_dir):
    """
    Load TPC-H tables into Thunderduck session (module-scoped).
    """
    tables = [
        'lineitem', 'orders', 'customer', 'part',
        'supplier', 'partsupp', 'nation', 'region'
    ]

    print("\nLoading TPC-H tables into Thunderduck...")
    for table in tables:
        parquet_path = tpch_data_dir / f"{table}.parquet"
        if not parquet_path.exists():
            pytest.skip(f"TPC-H table not found: {parquet_path}")

        df = spark_thunderduck.read.parquet(str(parquet_path))
        df.createOrReplaceTempView(table)

    print(f"✓ All {len(tables)} TPC-H tables loaded into Thunderduck")
    return tables


# ============================================================================
# TPC-DS Differential Testing Fixtures
# ============================================================================

# List of all TPC-DS tables
TPCDS_TABLES = [
    'call_center', 'catalog_page', 'catalog_returns', 'catalog_sales',
    'customer', 'customer_address', 'customer_demographics', 'date_dim',
    'household_demographics', 'income_band', 'inventory', 'item',
    'promotion', 'reason', 'ship_mode', 'store', 'store_returns', 'store_sales',
    'time_dim', 'warehouse', 'web_page', 'web_returns', 'web_sales', 'web_site'
]


@pytest.fixture(scope="module")
def tpcds_tables_reference(spark_reference, tpcds_data_dir):
    """
    Load TPC-DS tables into Spark Reference session (module-scoped).
    """
    print(f"\nLoading {len(TPCDS_TABLES)} TPC-DS tables into Spark Reference...")
    for table in TPCDS_TABLES:
        parquet_path = tpcds_data_dir / f"{table}.parquet"
        if not parquet_path.exists():
            pytest.skip(f"TPC-DS table not found: {parquet_path}")

        df = spark_reference.read.parquet(str(parquet_path))
        df.createOrReplaceTempView(table)

    print(f"✓ All {len(TPCDS_TABLES)} TPC-DS tables loaded into Spark Reference")
    return TPCDS_TABLES


@pytest.fixture(scope="module")
def tpcds_tables_thunderduck(spark_thunderduck, tpcds_data_dir):
    """
    Load TPC-DS tables into Thunderduck session (module-scoped).
    """
    print(f"\nLoading {len(TPCDS_TABLES)} TPC-DS tables into Thunderduck...")
    for table in TPCDS_TABLES:
        parquet_path = tpcds_data_dir / f"{table}.parquet"
        if not parquet_path.exists():
            pytest.skip(f"TPC-DS table not found: {parquet_path}")

        df = spark_thunderduck.read.parquet(str(parquet_path))
        df.createOrReplaceTempView(table)

    print(f"✓ All {len(TPCDS_TABLES)} TPC-DS tables loaded into Thunderduck")
    return TPCDS_TABLES
